{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32d6a8aa-c758-4993-b25a-d633bf37d482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file blockTransactions17011001-17012000.json...\n",
      "File size: 360.67 MB\n",
      "Processing blockTransactions17011001-17012000.json...\n",
      "Processed 1000 items: 7 blocks and 965 transactions\n",
      "Processed 2000 items: 13 blocks and 1947 transactions\n",
      "Processed 3000 items: 19 blocks and 2907 transactions\n",
      "Processed 4000 items: 26 blocks and 3849 transactions\n",
      "Processed 5000 items: 32 blocks and 4800 transactions\n",
      "Processed 6000 items: 40 blocks and 5752 transactions\n",
      "Processed 7000 items: 46 blocks and 6675 transactions\n",
      "Processed 8000 items: 52 blocks and 7594 transactions\n",
      "Processed 9000 items: 59 blocks and 8543 transactions\n",
      "Processed 10000 items: 65 blocks and 9513 transactions\n",
      "Processed 11000 items: 70 blocks and 10450 transactions\n",
      "Processed 12000 items: 75 blocks and 11429 transactions\n",
      "Processed 13000 items: 81 blocks and 12382 transactions\n",
      "Processed 14000 items: 88 blocks and 13339 transactions\n",
      "Processed 15000 items: 94 blocks and 14251 transactions\n",
      "Processed 16000 items: 100 blocks and 15220 transactions\n",
      "Processed 17000 items: 107 blocks and 16187 transactions\n",
      "Processed 18000 items: 114 blocks and 17076 transactions\n",
      "Processed 19000 items: 120 blocks and 18048 transactions\n",
      "Processed 20000 items: 127 blocks and 19002 transactions\n",
      "Processed 21000 items: 133 blocks and 19944 transactions\n",
      "Processed 22000 items: 140 blocks and 20873 transactions\n",
      "Processed 23000 items: 147 blocks and 21810 transactions\n",
      "Processed 24000 items: 154 blocks and 22767 transactions\n",
      "Processed 25000 items: 160 blocks and 23716 transactions\n",
      "Processed 26000 items: 168 blocks and 24627 transactions\n",
      "Processed 27000 items: 174 blocks and 25550 transactions\n",
      "Processed 28000 items: 181 blocks and 26516 transactions\n",
      "Processed 29000 items: 187 blocks and 27442 transactions\n",
      "Processed 30000 items: 193 blocks and 28389 transactions\n",
      "Processed 31000 items: 200 blocks and 29322 transactions\n",
      "Processed 32000 items: 206 blocks and 30261 transactions\n",
      "Processed 33000 items: 212 blocks and 31223 transactions\n",
      "Processed 34000 items: 218 blocks and 32170 transactions\n",
      "Processed 35000 items: 224 blocks and 33098 transactions\n",
      "Processed 36000 items: 229 blocks and 34069 transactions\n",
      "Processed 37000 items: 235 blocks and 34987 transactions\n",
      "Processed 38000 items: 241 blocks and 35932 transactions\n",
      "Processed 39000 items: 247 blocks and 36895 transactions\n",
      "Processed 40000 items: 254 blocks and 37812 transactions\n",
      "Processed 41000 items: 259 blocks and 38763 transactions\n",
      "Processed 42000 items: 266 blocks and 39719 transactions\n",
      "Processed 43000 items: 272 blocks and 40687 transactions\n",
      "Processed 44000 items: 280 blocks and 41663 transactions\n",
      "Processed 45000 items: 286 blocks and 42627 transactions\n",
      "Processed 46000 items: 292 blocks and 43550 transactions\n",
      "Processed 47000 items: 298 blocks and 44406 transactions\n",
      "Processed 48000 items: 303 blocks and 45332 transactions\n",
      "Processed 49000 items: 310 blocks and 46215 transactions\n",
      "Processed 50000 items: 316 blocks and 47195 transactions\n",
      "Processed 51000 items: 322 blocks and 48132 transactions\n",
      "Processed 52000 items: 330 blocks and 49115 transactions\n",
      "Processed 53000 items: 336 blocks and 50038 transactions\n",
      "\n",
      " Saving partial result\n",
      "\n",
      "Summary of processed files:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Saved complete summary to blockchain_files_complete_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_single_file(file_path, max_items=None, save_output=False):\n",
    "\n",
    "    print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "\n",
    "    block_count = 0\n",
    "    transaction_count = 0\n",
    "    blocks_data = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            parser = ijson.parse(f)\n",
    "            \n",
    "            current_block = None\n",
    "            item_type = None\n",
    "            item_data = {}\n",
    "            array_depth = 0\n",
    "            item_count = 0\n",
    "            progress_interval = 1000 \n",
    "            last_progress_time = time.time()\n",
    "            \n",
    "            for prefix, event, value in parser:\n",
    "                # Track array depth\n",
    "                if event == 'start_array':\n",
    "                    array_depth += 1\n",
    "                elif event == 'end_array':\n",
    "                    array_depth -= 1\n",
    "                    if array_depth == 1: \n",
    "                        current_block = None\n",
    "                \n",
    "                \n",
    "                if event == 'start_map':\n",
    "                    item_data = {}\n",
    "                    item_type = None\n",
    "                elif event == 'map_key':\n",
    "                    # check if this is a block or transaction\n",
    "                    if value == 'number' and prefix.count('.') == 1:\n",
    "                        item_type = 'block'\n",
    "                    elif value == 'blockNumber' and prefix.count('.') == 1:\n",
    "                        item_type = 'transaction'\n",
    "                elif event == 'end_map':\n",
    "                    if item_type == 'block':\n",
    "                        current_block = item_data.get('number')\n",
    "                        block_count += 1\n",
    "                        \n",
    "                        # get relevant block level data\n",
    "                        block_summary = {\n",
    "                            'block_number': item_data.get('number'),\n",
    "                            'timestamp': item_data.get('timestamp'),\n",
    "                            'gas_used': item_data.get('gasUsed'),\n",
    "                            'gas_limit': item_data.get('gasLimit'),\n",
    "                            'base_fee_per_gas': item_data.get('baseFeePerGas'),\n",
    "                            'transaction_count': len(item_data.get('transactions', [])) if 'transactions' in item_data else 0\n",
    "                        }\n",
    "                        blocks_data.append(block_summary)\n",
    "                        \n",
    "                        if save_output and len(blocks_data) >= 1000:\n",
    "                            partial_df = pd.DataFrame(blocks_data)\n",
    "                            output_file = os.path.basename(file_path).replace('.json', f'_blocks_{block_count-len(blocks_data)}_to_{block_count}.csv')\n",
    "                            partial_df.to_csv(output_file, index=False)\n",
    "                            print(f\"Saved partial block data to {output_file}\")\n",
    "                            blocks_data = [] \n",
    "                        \n",
    "                    elif item_type == 'transaction':\n",
    "                        transaction_count += 1\n",
    "                    \n",
    "                    item_count += 1\n",
    "                    \n",
    "                    current_time = time.time()\n",
    "                    if item_count % progress_interval == 0 or current_time - last_progress_time > 30:\n",
    "                        print(f\"Processed {item_count} items: {block_count} blocks and {transaction_count} transactions\")\n",
    "                        last_progress_time = current_time\n",
    "                    \n",
    "                    if max_items and item_count >= max_items:\n",
    "                        break\n",
    "                \n",
    "\n",
    "                elif prefix.count('.') == 2 and event != 'start_array' and event != 'end_array':\n",
    "                    field = prefix.split('.')[-1]\n",
    "                    item_data[field] = value\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            if save_output and blocks_data:\n",
    "                final_df = pd.DataFrame(blocks_data)\n",
    "                output_file = os.path.basename(file_path).replace('.json', '_final_blocks.csv')\n",
    "                final_df.to_csv(output_file, index=False)\n",
    "                print(f\"Saved final block data to {output_file}\")\n",
    "\n",
    "\n",
    "            \n",
    "            if blocks_data:\n",
    "                df = pd.DataFrame(blocks_data)\n",
    "                \n",
    "                # Calculate stat\n",
    "                avg_gas_used = df['gas_used'].mean() if 'gas_used' in df.columns else 0\n",
    "                avg_tx_per_block = df['transaction_count'].mean() if 'transaction_count' in df.columns else 0\n",
    "                \n",
    "                \n",
    "                block_range = f\"{df['block_number'].min()} - {df['block_number'].max()}\" if 'block_number' in df.columns and not df.empty else \"Unknown\"\n",
    "                \n",
    "                return {\n",
    "                    'file': os.path.basename(file_path),\n",
    "                    'block_count': block_count,\n",
    "                    'transaction_count': transaction_count,\n",
    "                    'avg_gas_used': avg_gas_used,\n",
    "                    'avg_transactions_per_block': avg_tx_per_block,\n",
    "                    'block_range': block_range,\n",
    "                    'processing_complete': max_items is None\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'file': os.path.basename(file_path),\n",
    "                'block_count': block_count,\n",
    "                'transaction_count': transaction_count,\n",
    "                'note': \"No block data was collected\",\n",
    "                'processing_complete': max_items is None\n",
    "            }\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return {\n",
    "            'file': os.path.basename(file_path),\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def chunked_json_reader(file_path, chunk_size=1000):\n",
    "    with open(file_path, 'r') as f:\n",
    "\n",
    "        char = f.read(1)\n",
    "        while char != '[':\n",
    "            char = f.read(1)\n",
    "            if not char:\n",
    "                return\n",
    "        \n",
    "\n",
    "        char = f.read(1)\n",
    "        while char != '[':\n",
    "            char = f.read(1)\n",
    "            if not char:\n",
    "                return\n",
    "        \n",
    " \n",
    "        buffer = \"\"\n",
    "        brace_count = 0\n",
    "        in_string = False\n",
    "        escape_next = False\n",
    "        \n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "                \n",
    "            for char in chunk:\n",
    "  \n",
    "                if char == '\"' and not escape_next:\n",
    "                    in_string = not in_string\n",
    "                elif char == '\\\\' and in_string and not escape_next:\n",
    "                    escape_next = True\n",
    "                    buffer += char\n",
    "                    continue\n",
    "                else:\n",
    "                    escape_next = False\n",
    "                \n",
    "                buffer += char\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                if not in_string:\n",
    "                    if char == '{':\n",
    "                        brace_count += 1\n",
    "                    elif char == '}':\n",
    "                        brace_count -= 1\n",
    "                        if brace_count == 0:\n",
    "\n",
    "                            try:\n",
    "                                yield json.loads(buffer)\n",
    "                            except json.JSONDecodeError:\n",
    "                                print(f\"Error decoding JSON object: {buffer[:100]}...\")\n",
    "                            \n",
    "                            buffer = \"\"\n",
    "                            \n",
    "                            char = f.read(1)\n",
    "                            while char and char not in [',', ']']:\n",
    "                                char = f.read(1)\n",
    "                            \n",
    "                            if char == ']': \n",
    "                                char = f.read(1)\n",
    "                                while char and char not in ['[', ']']:\n",
    "                                    char = f.read(1)\n",
    "                                \n",
    "                                if char != '[': \n",
    "                                    return\n",
    "\n",
    "def process_all_files(file_list, sample_only=True, max_items_per_file=100):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\nProcessing file {os.path.basename(file_path)}...\")\n",
    "            print(f\"File size: {os.path.getsize(file_path) / (1024 * 1024):.2f} MB\")\n",
    "                        \n",
    "            try:\n",
    "                result = process_single_file(\n",
    "                    file_path, \n",
    "                    max_items=max_items_per_file if sample_only else None,\n",
    "                    save_output=not sample_only\n",
    "                )\n",
    "                results.append(result)\n",
    "                \n",
    "         \n",
    "                print(f\"Completed processing {os.path.basename(file_path)}\")\n",
    "                print(f\"Found {result.get('block_count', 0)} blocks and {result.get('transaction_count', 0)} transactions\")\n",
    "                \n",
    "\n",
    "                if len(results) > 0:\n",
    "                    intermediate_df = pd.DataFrame(results)\n",
    "                    intermediate_df.to_csv(f\"blockchain_processing_progress_{len(results)}_files.csv\", index=False)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n Saving partial result\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                results.append({\n",
    "                    'file': os.path.basename(file_path),\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    print(\"\\nSummary of processed files:\")\n",
    "    print(summary_df)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def extract_specific_data(file_path, target_block=None, target_tx=None, max_search_items=1000):\n",
    "    print(f\"Searching in {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    try:\n",
    "        item_count = 0\n",
    "        \n",
    "        for item in chunked_json_reader(file_path):\n",
    "            if target_block is not None and 'number' in item and item['number'] == target_block:\n",
    "                return {'type': 'block', 'data': item}\n",
    "            \n",
    "            if target_tx is not None and 'hash' in item and item['hash'] == target_tx:\n",
    "                return {'type': 'transaction', 'data': item}\n",
    "            \n",
    "            item_count += 1\n",
    "            if item_count >= max_search_items:\n",
    "                break\n",
    "                \n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = [\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17011001-17012000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17015001-17020000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17010001-17011000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17012001-17015000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17030001-17050000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17020001-17030000-006.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17090001-17100000-005.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17000000-17010000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/new_blockTransactions17175001-17200000-002.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17150001-17175000-005.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17100000-17125000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17125001-17150000-003.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17175001-17200000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17275001-17300000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17225001-17250000-003.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17200000-17225000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17350001-17400000-002.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17300001-17350000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17250001-17275000-002.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17400001-17450000\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17450001-17500000\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    summary = process_all_files(files, sample_only=False)\n",
    "    \n",
    "\n",
    "    summary.to_csv(\"blockchain_files_complete_summary.csv\", index=False)\n",
    "    print(f\"Saved complete summary to blockchain_files_complete_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7182809f-15a9-422f-aeef-d7a3ebd6daf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 block data files\n",
      "Processing chunk 1 with 38 files\n",
      "Processed 38 files in this chunk\n",
      "Combined block data saved to combined_blockchain_blocks.csv\n",
      "\n",
      "Sample of combined data:\n",
      "   block_number   timestamp  gas_used  gas_limit  base_fee_per_gas  transaction_count                                        source_file\n",
      "0      17000000  1680911891   9160778   30000000       20582738913                  0  blockTransactions17000000-17010000-001_blocks_...\n",
      "1      17000001  1680911903   9389175   30000000       19581179064                  0  blockTransactions17000000-17010000-001_blocks_...\n",
      "2      17000002  1680911915  29993802   30000000       18665624323                  0  blockTransactions17000000-17010000-001_blocks_...\n",
      "3      17000003  1680911927  11343154   30000000       20997863283                  0  blockTransactions17000000-17010000-001_blocks_...\n",
      "4      17000004  1680911939   7688030   30000000       20357980347                  0  blockTransactions17000000-17010000-001_blocks_...\n",
      "Combined file size: 3.46 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def combine_block_data_files(folder_paths, output_file='combined_blockchain_blocks.csv'):\n",
    "\n",
    "\n",
    "    all_files = []\n",
    "    \n",
    "\n",
    "    for folder in folder_paths:\n",
    "        block_files = glob.glob(os.path.join(folder, '*blocks*.csv'))\n",
    "        block_files = [f for f in block_files if 'processing_progress' not in f and 'summary' not in f]\n",
    "        \n",
    "        all_files.extend(block_files)\n",
    "    \n",
    "    print(f\"Found {len(all_files)} block data files\")\n",
    "    \n",
    "\n",
    "    chunk_size = 50 \n",
    "    for i in range(0, len(all_files), chunk_size):\n",
    "        chunk_files = all_files[i:i+chunk_size]\n",
    "        print(f\"Processing chunk {i//chunk_size + 1} with {len(chunk_files)} files\")\n",
    "        \n",
    "\n",
    "        dfs = []\n",
    "        \n",
    "        for file in chunk_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                \n",
    "                \n",
    "                df['source_file'] = os.path.basename(file)\n",
    "                \n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {str(e)}\")\n",
    "        \n",
    "        if dfs:\n",
    "\n",
    "            chunk_df = pd.concat(dfs, ignore_index=True)\n",
    "            \n",
    "    \n",
    "            if i == 0:\n",
    "                chunk_df.to_csv(output_file, index=False)\n",
    "            else:\n",
    "                chunk_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            \n",
    "            print(f\"Processed {len(dfs)} files in this chunk\")\n",
    "            \n",
    "\n",
    "            del dfs\n",
    "            del chunk_df\n",
    "        \n",
    "    print(f\"Combined block data saved to {output_file}\")\n",
    "\n",
    "    return pd.read_csv(output_file, nrows=1000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folders = [\n",
    "        \"C:/Users/haile/Downloads/Eth datasets csv/1st\",\n",
    "        \"C:/Users/haile/Downloads/Eth datasets csv/2nd\",\n",
    "        \"C:/Users/haile/Downloads/Eth datasets csv/3rd\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    sample_data = combine_block_data_files(folders)\n",
    "    \n",
    "\n",
    "    print(\"\\nSample of combined data:\")\n",
    "    print(sample_data.head())\n",
    "\n",
    "    \n",
    "    output_size_mb = os.path.getsize('combined_blockchain_blocks.csv') / (1024 * 1024)\n",
    "    print(f\"Combined file size: {output_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea620eb-9b7b-4c50-a497-74bbd376a3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
