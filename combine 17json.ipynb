{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf521fc2-f636-459a-8732-7754e8fb76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import polars as pl\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import ijson\n",
    "\n",
    "def extract_blockchain_data(file_path, max_blocks=None, batch_size=1000):\n",
    "    print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    blocks_data = []\n",
    "    transactions_data = []\n",
    "    \n",
    "    blocks_df_list = []\n",
    "    transactions_df_list = []\n",
    "    \n",
    "    block_count = 0\n",
    "    transaction_count = 0\n",
    "    current_batch = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            parser = ijson.parse(f)\n",
    "            \n",
    "            current_block = None\n",
    "            current_block_number = None\n",
    "            current_block_hash = None\n",
    "            current_block_timestamp = None\n",
    "            current_block_transactions = 0\n",
    "            array_depth = 0\n",
    "            last_progress_time = time.time()\n",
    "            inner_array_started = False\n",
    "            \n",
    "            item_type = None\n",
    "            item_data = {}\n",
    "            \n",
    "            for prefix, event, value in parser:               \n",
    "                if event == 'start_array':\n",
    "                    array_depth += 1\n",
    "                    if array_depth == 2:\n",
    "                        inner_array_started = True\n",
    "                        current_block_transactions = 0\n",
    "                        \n",
    "                elif event == 'end_array':\n",
    "                    if array_depth == 2:\n",
    "                        inner_array_started = False\n",
    "                        if current_block is not None and len(blocks_data) > 0:\n",
    "                            blocks_data[-1]['transaction_count'] = current_block_transactions\n",
    "                    \n",
    "                    array_depth -= 1\n",
    "                    if array_depth == 1:\n",
    "                        current_block = None\n",
    "                        current_block_number = None\n",
    "                        current_block_hash = None\n",
    "                        current_block_timestamp = None\n",
    "                \n",
    "                if event == 'start_map':\n",
    "                    item_data = {}\n",
    "                    item_type = None\n",
    "                elif event == 'map_key':\n",
    "                    # check block or transaction\n",
    "                    if value == 'number' and prefix.count('.') == 1:\n",
    "                        item_type = 'block'\n",
    "                    elif value == 'blockNumber' and prefix.count('.') == 1:\n",
    "                        item_type = 'transaction'\n",
    "                elif event == 'end_map':\n",
    "                    if item_type == 'block':\n",
    "                        # block data\n",
    "                        block_summary = {\n",
    "                            'block_number': item_data.get('number'),\n",
    "                            'timestamp': item_data.get('timestamp'),\n",
    "                            'gas_used': item_data.get('gasUsed'),\n",
    "                            'gas_limit': item_data.get('gasLimit'),\n",
    "                            'base_fee_per_gas': item_data.get('baseFeePerGas'),\n",
    "                            'block_hash': item_data.get('hash'),\n",
    "                            'transaction_count': 0,\n",
    "                            'miner': item_data.get('miner'),\n",
    "                            'size': item_data.get('size'),\n",
    "                            'parent_hash': item_data.get('parentHash'),\n",
    "                            'receipts_root': item_data.get('receiptsRoot'),\n",
    "                            'state_root': item_data.get('stateRoot'),\n",
    "                            'transactions_root': item_data.get('transactionsRoot')\n",
    "                        }\n",
    "                        blocks_data.append(block_summary)\n",
    "                        current_block = block_summary\n",
    "                        current_block_number = item_data.get('number')\n",
    "                        current_block_hash = item_data.get('hash')\n",
    "                        current_block_timestamp = item_data.get('timestamp')\n",
    "                        block_count += 1\n",
    "                        \n",
    "                    elif item_type == 'transaction':\n",
    "                        if inner_array_started:\n",
    "                            current_block_transactions += 1\n",
    "                        \n",
    "                        tx_timestamp = None\n",
    "                        if current_block_number == item_data.get('blockNumber'):\n",
    "                            tx_timestamp = current_block_timestamp\n",
    "                        \n",
    "                        #  transaction data\n",
    "                        transaction_summary = {\n",
    "                            'tx_hash': item_data.get('hash'),\n",
    "                            'block_number': item_data.get('blockNumber'),\n",
    "                            'block_hash': item_data.get('blockHash'),\n",
    "                            'timestamp': tx_timestamp,\n",
    "                            'from_address': item_data.get('from'),\n",
    "                            'to_address': item_data.get('to'),\n",
    "                            'value': item_data.get('value'),\n",
    "                            'gas': item_data.get('gas'),\n",
    "                            'gas_price': item_data.get('gasPrice'),\n",
    "                            'tx_type': item_data.get('type'),\n",
    "                            'nonce': item_data.get('nonce'),\n",
    "                            'transaction_index': item_data.get('transactionIndex'),\n",
    "                            'chain_id': item_data.get('chainId')\n",
    "                        }\n",
    "                            \n",
    "                        transactions_data.append(transaction_summary)\n",
    "                        transaction_count += 1\n",
    "                    \n",
    "                    #  progress\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_progress_time > 30:\n",
    "                        print(f\"Processed {block_count} blocks and {transaction_count} transactions. Current batch: {current_batch}\")\n",
    "                        last_progress_time = current_time\n",
    "\n",
    "                    if block_count > 0 and block_count % batch_size == 0 and len(blocks_data) > 0:\n",
    "                        batch_blocks_df = pl.DataFrame(blocks_data)\n",
    "                        batch_transactions_df = pl.DataFrame(transactions_data)\n",
    "                        \n",
    "                        blocks_df_list.append(batch_blocks_df)\n",
    "                        transactions_df_list.append(batch_transactions_df)\n",
    "                        \n",
    "                        blocks_data = []\n",
    "                        transactions_data = []\n",
    "                        current_batch += 1\n",
    "                        \n",
    "                        print(f\"Batch {current_batch} processed: {batch_size} blocks, {len(batch_transactions_df)} transactions\")\n",
    "                    \n",
    "                    if max_blocks and block_count >= max_blocks:\n",
    "                        break\n",
    "                \n",
    "                elif prefix.count('.') == 2 and event != 'start_array' and event != 'end_array':\n",
    "                    field = prefix.split('.')[-1]\n",
    "                    item_data[field] = value\n",
    "            \n",
    "            if len(blocks_data) > 0:\n",
    "                batch_blocks_df = pl.DataFrame(blocks_data)\n",
    "                batch_transactions_df = pl.DataFrame(transactions_data)\n",
    "                \n",
    "                blocks_df_list.append(batch_blocks_df)\n",
    "                transactions_df_list.append(batch_transactions_df)\n",
    "                \n",
    "                print(f\"Final batch processed: {len(blocks_data)} blocks, {len(transactions_data)} transactions\")\n",
    "            \n",
    "            if blocks_df_list:\n",
    "                blocks_df = pl.concat(blocks_df_list)\n",
    "            else:\n",
    "                blocks_df = pl.DataFrame()\n",
    "                \n",
    "            if transactions_df_list:\n",
    "                transactions_df = pl.concat(transactions_df_list)\n",
    "            else:\n",
    "                transactions_df = pl.DataFrame()\n",
    "            \n",
    "            print(f\"Completed processing {os.path.basename(file_path)}\")\n",
    "            print(f\"Extracted {block_count} blocks and {transaction_count} transactions\")\n",
    "\n",
    "            #missing timestamp\n",
    "            if not transactions_df.is_empty() and 'timestamp' in transactions_df.columns:\n",
    "                missing_timestamps = transactions_df.filter(pl.col('timestamp').is_null()).height\n",
    "                \n",
    "                if missing_timestamps > 0 and not blocks_df.is_empty():\n",
    "                    print(f\"Filling in {missing_timestamps} missing timestamps...\")\n",
    "                    \n",
    "                    block_timestamp_map = blocks_df.select(['block_number', 'timestamp']).unique().to_dict(as_series=False)\n",
    "                    block_ts_dict = dict(zip(block_timestamp_map['block_number'], block_timestamp_map['timestamp']))\n",
    "                    \n",
    "                    def fill_timestamp(row):\n",
    "                        if row['timestamp'] is None and row['block_number'] in block_ts_dict:\n",
    "                            return block_ts_dict[row['block_number']]\n",
    "                        return row['timestamp']\n",
    "                    \n",
    "                    transactions_df = transactions_df.with_columns(\n",
    "                        pl.struct(['timestamp', 'block_number'])\n",
    "                        .map_elements(lambda x: fill_timestamp(x), return_dtype=pl.Int64)\n",
    "                        .alias('timestamp')\n",
    "                    )\n",
    "                    \n",
    "                    # check missing timestamps\n",
    "                    still_missing = transactions_df.filter(pl.col('timestamp').is_null()).height\n",
    "                    if still_missing > 0:\n",
    "                        print(f\"Warning: {still_missing} transactions still missing timestamps\")\n",
    "                    else:\n",
    "                        print(\"All timestamps filled successfully\")\n",
    "            \n",
    "            return blocks_df, transactions_df\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return pl.DataFrame(), pl.DataFrame()\n",
    "\n",
    "def process_multiple_files(file_list, max_blocks_per_file=None, batch_size=1000):\n",
    "    all_blocks_list = []\n",
    "    all_transactions_list = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\nProcessing file {os.path.basename(file_path)}...\")\n",
    "            print(f\"File size: {os.path.getsize(file_path) / (1024 * 1024):.2f} MB\")\n",
    "            \n",
    "            try:\n",
    "                blocks_df, transactions_df = extract_blockchain_data(\n",
    "                    file_path, \n",
    "                    max_blocks=max_blocks_per_file,\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "                \n",
    "                if not blocks_df.is_empty():\n",
    "                    all_blocks_list.append(blocks_df)\n",
    "                if not transactions_df.is_empty():\n",
    "                    all_transactions_list.append(transactions_df)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nProcessing interrupted by user.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    if all_blocks_list:\n",
    "        combined_blocks = pl.concat(all_blocks_list)\n",
    "        # remove duplicates\n",
    "        combined_blocks = combined_blocks.unique(subset=['block_number', 'block_hash'])\n",
    "    else:\n",
    "        combined_blocks = pl.DataFrame()\n",
    "        \n",
    "    if all_transactions_list:\n",
    "        combined_transactions = pl.concat(all_transactions_list)\n",
    "        # remove duplicates\n",
    "        combined_transactions = combined_transactions.unique(subset=['tx_hash'])\n",
    "    else:\n",
    "        combined_transactions = pl.DataFrame()\n",
    "    \n",
    "    print(\"\\n=== Blockchain Data Processing Summary ===\")\n",
    "    if not combined_blocks.is_empty():\n",
    "        print(f\"Total unique blocks: {combined_blocks.height}\")\n",
    "        min_block = combined_blocks['block_number'].min()\n",
    "        max_block = combined_blocks['block_number'].max()\n",
    "        print(f\"Block range: {min_block} - {max_block}\")\n",
    "    \n",
    "    if not combined_transactions.is_empty():\n",
    "        print(f\"Total unique transactions: {combined_transactions.height}\")\n",
    "    \n",
    "    return combined_blocks, combined_transactions\n",
    "\n",
    "def analyze_blockchain_data(blocks_df, transactions_df):\n",
    "    results = {}\n",
    "    \n",
    "    if not blocks_df.is_empty():\n",
    "        # Block-level analysis\n",
    "        results['block_count'] = blocks_df.height\n",
    "        results['block_range'] = f\"{blocks_df['block_number'].min()} - {blocks_df['block_number'].max()}\"\n",
    "        results['avg_gas_used'] = blocks_df['gas_used'].mean()\n",
    "        results['avg_gas_limit'] = blocks_df['gas_limit'].mean()\n",
    "        results['gas_utilization'] = (blocks_df['gas_used'] / blocks_df['gas_limit']).mean() * 100\n",
    "        results['avg_block_size'] = blocks_df['size'].mean()\n",
    "        \n",
    "        # Time analysis\n",
    "        if 'timestamp' in blocks_df.columns:\n",
    "            blocks_df_sorted = blocks_df.sort('block_number')\n",
    "            blocks_df_sorted = blocks_df_sorted.with_columns(\n",
    "                pl.col('timestamp').diff().alias('time_diff')\n",
    "            )\n",
    "            results['avg_block_time'] = blocks_df_sorted['time_diff'].mean()\n",
    "    \n",
    "    if not transactions_df.is_empty():\n",
    "        # Transaction-level analysis\n",
    "        results['transaction_count'] = transactions_df.height\n",
    "        \n",
    "        tx_per_block = transactions_df.group_by('block_number').agg(pl.len().alias('tx_count'))\n",
    "        results['total_transactions_from_tx_data'] = tx_per_block['tx_count'].sum()\n",
    "        results['avg_transactions_per_block'] = tx_per_block['tx_count'].mean()\n",
    "        results['max_transactions_in_a_block'] = tx_per_block['tx_count'].max()\n",
    "        \n",
    "        #  avg gas price\n",
    "        if 'gas_price' in transactions_df.columns:\n",
    "            if transactions_df['gas_price'].dtype == pl.Utf8:\n",
    "                if transactions_df['gas_price'].head(1)[0].startswith('0x'):\n",
    "                    transactions_df = transactions_df.with_columns(\n",
    "                        pl.col('gas_price').map_elements(lambda x: int(x, 16) if isinstance(x, str) and x.startswith('0x') else x).alias('gas_price_numeric')\n",
    "                    )\n",
    "                else:\n",
    "                    transactions_df = transactions_df.with_columns(\n",
    "                        pl.col('gas_price').cast(pl.Float64, strict=False).alias('gas_price_numeric')\n",
    "                    )\n",
    "                results['avg_gas_price'] = transactions_df['gas_price_numeric'].mean()\n",
    "            else:\n",
    "                results['avg_gas_price'] = transactions_df['gas_price'].mean()\n",
    "    \n",
    "    if not blocks_df.is_empty() and not transactions_df.is_empty():\n",
    "        blocks_with_tx = set(blocks_df['block_number'].to_list())\n",
    "        tx_blocks = set(transactions_df['block_number'].to_list())\n",
    "        \n",
    "        results['blocks_with_transactions'] = len(blocks_with_tx.intersection(tx_blocks))\n",
    "        results['blocks_missing_transactions'] = len(blocks_with_tx - tx_blocks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def show_dataframe(df, name=\"DataFrame\"):\n",
    "\n",
    "    if df.is_empty():\n",
    "        print(f\"{name} is empty\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n===== {name} Information =====\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = [\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17000000-17010000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17010001-17011000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17011001-17012000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17012001-17015000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17015001-17020000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17020001-17030000-006.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17030001-17050000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17090001-17100000-005.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17100000-17125000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17125001-17150000-003.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17150001-17175000-005.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17175001-17200000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17200000-17225000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17225001-17250000-003.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17250001-17275000-002.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17275001-17300000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17300001-17350000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17350001-17400000-002.json\"\n",
    "    ]\n",
    "    \n",
    "    blocks_df, transactions_df = process_multiple_files(\n",
    "        files, \n",
    "        max_blocks_per_file=None,\n",
    "        batch_size=1000          \n",
    "    )\n",
    "    \n",
    "    show_dataframe(blocks_df, \"Blocks DataFrame\")\n",
    "    show_dataframe(transactions_df, \"Transactions DataFrame\")\n",
    "    \n",
    "    if not blocks_df.is_empty() or not transactions_df.is_empty():\n",
    "        analysis = analyze_blockchain_data(blocks_df, transactions_df)\n",
    "        \n",
    "        print(\"\\n===== Blockchain Data Analysis =====\")\n",
    "        for key, value in analysis.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972e257-508c-4cae-86ed-33e83568b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17000000-17010000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17010001-17011000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17011001-17012000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17012001-17015000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17015001-17020000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17020001-17030000-006.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17030001-17050000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17090001-17100000-005.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17100000-17125000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17125001-17150000-003.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17150001-17175000-005.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17175001-17200000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17200000-17225000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17225001-17250000-003.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17250001-17275000-002.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17275001-17300000-004.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17300001-17350000-001.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17350001-17400000-002.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17400001-17450000\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17450001-17500000\"\n",
    "        \"C:/Users/haile/Downloads/170-171/new_blockTransactions17175001-17200000-002.json\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "00444b5a-f24e-46a9-ba21-14fc0a5c8f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file blockTransactions17011001-17012000.json...\n",
      "File size: 360.67 MB\n",
      "Processing blockTransactions17011001-17012000.json...\n",
      "Completed processing blockTransactions17011001-17012000.json\n",
      "Extracted 1000 blocks and 148663 transactions\n",
      "Saved block data to blockTransactions17011001-17012000_blocks.csv\n",
      "Saved transaction data to blockTransactions17011001-17012000_transactions.csv\n",
      "\n",
      "Processing file blockTransactions17015001-17020000.json...\n",
      "File size: 1581.54 MB\n",
      "Processing blockTransactions17015001-17020000.json...\n",
      "Processed 2428 blocks and 325990 transactions. Current block: {'block_number': 17017428, 'timestamp': 1681125167, 'gas_used': 21393778, 'gas_limit': 30000000, 'base_fee_per_gas': '19797934507', 'block_hash': '0x5b0efaa0b246ea6b2dd34830215dafdbe702228822882695f45b2e25beb2e23a', 'transaction_count': 0, 'miner': '0xdafea492d9c6733ae3d56b7ed1adb60692c98bc5', 'size': 92779, 'parent_hash': '0x267a9afdeea8867935a5341909d6ef701db413ea07910449a14217c0c0bc0c9a', 'receipts_root': '0x9caa3bcf0238eb8d4b144cf7ba51478d9d0c09f99fe1a965e59da18c9fcbdef5', 'state_root': '0x50f627ab1551775b67ce11219e68e7d2785219f10356facad57344b8bb62e8af', 'transactions_root': '0x67a6df71c8134534c2cd3bc396565102c7fd3ed4669a90c1a139b31db25d0128'}\n",
      "Processed 4753 blocks and 690745 transactions. Current block: {'block_number': 17019753, 'timestamp': 1681153751, 'gas_used': 10986031, 'gas_limit': 30000000, 'base_fee_per_gas': '32923492509', 'block_hash': '0xbf3e7fd92890abc5ce388fbd5acb589b1c66319dec43e1a62b4b839a4e81de5e', 'transaction_count': 0, 'miner': '0x95222290dd7278aa3ddd389cc1e1d165cc4bafe5', 'size': 43353, 'parent_hash': '0x43a036e36a743b3adb8a63d4fb59040570872ba33768e069fc296b4d3c898575', 'receipts_root': '0xe60109907356579b53dc2a1205a21e483fc20bc6b15058a86004b32d9164564c', 'state_root': '0xaf105a551fe699678d87d52bbc14ec5b10914a5117cb18f1535a4c723c315325', 'transactions_root': '0x95de2e190cc2a45cc6919aceb34e9b0330263c2cad5ad12f5697843cc726304b'}\n",
      "Completed processing blockTransactions17015001-17020000.json\n",
      "Extracted 5000 blocks and 728431 transactions\n",
      "Saved block data to blockTransactions17015001-17020000_blocks.csv\n",
      "Saved transaction data to blockTransactions17015001-17020000_transactions.csv\n",
      "Saved combined block data: 6000 unique blocks\n",
      "Saved combined transaction data: 877094 unique transactions\n",
      "\n",
      "Blockchain Data Analysis:\n",
      "block_count: 6000\n",
      "block_range: 17011001 - 17020000\n",
      "avg_gas_used: 15141304.4635\n",
      "avg_gas_limit: 29999863.159333333\n",
      "gas_utilization: 50.47127198766721\n",
      "avg_block_size: 127603.56533333333\n",
      "avg_block_time: 18.391065177529587\n",
      "transaction_count: 877094\n",
      "total_transactions_from_tx_data: 877094\n",
      "avg_transactions_per_block: 146.4263772954925\n",
      "max_transactions_in_a_block: 815\n",
      "avg_gas_price: 27825303836.210487\n",
      "avg_transaction_value: 9.68119949036882e+17\n",
      "total_transaction_value: 8.49132198580555e+23\n",
      "zero_value_tx_percentage: 53.50692172104701\n",
      "blocks_with_transactions: 5990\n",
      "blocks_missing_transactions: 10\n",
      "block_reported_tx_count: 877094\n",
      "actual_tx_count: 877094\n",
      "tx_count_match: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import ijson\n",
    "\n",
    "def extract_blockchain_data(file_path, max_blocks=None, save_output=False):\n",
    "    print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "\n",
    "    blocks_data = []\n",
    "    transactions_data = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "\n",
    "            parser = ijson.parse(f)\n",
    "            \n",
    "\n",
    "            current_block = None\n",
    "            current_block_number = None\n",
    "            current_block_hash = None\n",
    "            current_block_transactions = 0\n",
    "            block_count = 0\n",
    "            transaction_count = 0\n",
    "            array_depth = 0\n",
    "            last_progress_time = time.time()\n",
    "            inner_array_started = False\n",
    "            \n",
    "            item_type = None\n",
    "            item_data = {}\n",
    "            \n",
    "            for prefix, event, value in parser:\n",
    "\n",
    "                if event == 'start_array':\n",
    "                    array_depth += 1\n",
    "                    if array_depth == 2:\n",
    "                        inner_array_started = True\n",
    "                        current_block_transactions = 0 \n",
    "                        \n",
    "                elif event == 'end_array':\n",
    "                    if array_depth == 2:\n",
    "                        inner_array_started = False\n",
    "                        if current_block is not None and len(blocks_data) > 0:\n",
    "                            blocks_data[-1]['transaction_count'] = current_block_transactions\n",
    "                    \n",
    "                    array_depth -= 1\n",
    "                    if array_depth == 1: \n",
    "                        current_block = None\n",
    "                        current_block_number = None\n",
    "                        current_block_hash = None\n",
    "                \n",
    "\n",
    "                if event == 'start_map':\n",
    "                    item_data = {}\n",
    "                    item_type = None\n",
    "                elif event == 'map_key':\n",
    "                    # check if this is a block or transaction\n",
    "                    if value == 'number' and prefix.count('.') == 1:\n",
    "                        item_type = 'block'\n",
    "                    elif value == 'blockNumber' and prefix.count('.') == 1:\n",
    "                        item_type = 'transaction'\n",
    "                elif event == 'end_map':\n",
    "                    if item_type == 'block':\n",
    "                        #block data \n",
    "                        block_summary = {\n",
    "                            'block_number': item_data.get('number'),\n",
    "                            'timestamp': item_data.get('timestamp'),\n",
    "                            'gas_used': item_data.get('gasUsed'),\n",
    "                            'gas_limit': item_data.get('gasLimit'),\n",
    "                            'base_fee_per_gas': item_data.get('baseFeePerGas'),\n",
    "                            'block_hash': item_data.get('hash'),\n",
    "                            'transaction_count': 0,\n",
    "                            'miner': item_data.get('miner'),\n",
    "                            'size': item_data.get('size'),\n",
    "                            'parent_hash': item_data.get('parentHash'),\n",
    "                            'receipts_root': item_data.get('receiptsRoot'),\n",
    "                            'state_root': item_data.get('stateRoot'),\n",
    "                            'transactions_root': item_data.get('transactionsRoot')\n",
    "                        }\n",
    "                        blocks_data.append(block_summary)\n",
    "                        current_block = block_summary\n",
    "                        current_block_number = item_data.get('number')\n",
    "                        current_block_hash = item_data.get('hash')\n",
    "                        block_count += 1\n",
    "                        \n",
    "                    elif item_type == 'transaction':\n",
    "                        if inner_array_started:\n",
    "                            current_block_transactions += 1\n",
    "                        \n",
    "                        block_timestamp = None\n",
    "\n",
    "                        if current_block is not None and current_block_number == item_data.get('blockNumber'):\n",
    "                            block_timestamp = current_block.get('timestamp')\n",
    "                        elif blocks_data:\n",
    "                            # matching block by number or hash\n",
    "                            for block in blocks_data:\n",
    "                                if (block['block_number'] == item_data.get('blockNumber') or \n",
    "                                    block['block_hash'] == item_data.get('blockHash')):\n",
    "                                    block_timestamp = block['timestamp']\n",
    "                                    break\n",
    "                        \n",
    "                        # transaction data\n",
    "                        transaction_summary = {\n",
    "                            'tx_hash': item_data.get('hash'),\n",
    "                            'block_number': item_data.get('blockNumber'),\n",
    "                            'block_hash': item_data.get('blockHash'),\n",
    "                            'timestamp': block_timestamp, \n",
    "                            'from_address': item_data.get('from'),\n",
    "                            'to_address': item_data.get('to'),\n",
    "                            'value': item_data.get('value'),\n",
    "                            'gas': item_data.get('gas'),\n",
    "                            'gas_price': item_data.get('gasPrice'),\n",
    "                            'tx_type': item_data.get('type'),\n",
    "                            'nonce': item_data.get('nonce'),\n",
    "                            'transaction_index': item_data.get('transactionIndex'),\n",
    "                            'chain_id': item_data.get('chainId')\n",
    "                        }\n",
    "\n",
    "                        \n",
    "                            \n",
    "                        transactions_data.append(transaction_summary)\n",
    "                        transaction_count += 1\n",
    "                    \n",
    "\n",
    "                    current_time = time.time()\n",
    "                    if current_block is not None and (current_time - last_progress_time > 30):\n",
    "                        print(f\"Processed {block_count} blocks and {transaction_count} transactions. Current block: {current_block}\")\n",
    "                        last_progress_time = current_time\n",
    "                    \n",
    "                    if max_blocks and block_count >= max_blocks:\n",
    "                        break\n",
    "                \n",
    "\n",
    "                elif prefix.count('.') == 2 and event != 'start_array' and event != 'end_array':\n",
    "                    field = prefix.split('.')[-1]\n",
    "                    item_data[field] = value\n",
    "            \n",
    "            print(f\"Completed processing {os.path.basename(file_path)}\")\n",
    "            print(f\"Extracted {block_count} blocks and {transaction_count} transactions\")\n",
    "            \n",
    "\n",
    "            blocks_df = pd.DataFrame(blocks_data) if blocks_data else pd.DataFrame()\n",
    "            transactions_df = pd.DataFrame(transactions_data) if transactions_data else pd.DataFrame()\n",
    "            \n",
    "            # save to CSV\n",
    "            if save_output and not blocks_df.empty:\n",
    "                blocks_output_file = os.path.basename(file_path).replace('.json', '_blocks.csv')\n",
    "                blocks_df.to_csv(blocks_output_file, index=False)\n",
    "                print(f\"Saved block data to {blocks_output_file}\")\n",
    "                \n",
    "                if not transactions_df.empty:\n",
    "                    tx_output_file = os.path.basename(file_path).replace('.json', '_transactions.csv')\n",
    "                    transactions_df.to_csv(tx_output_file, index=False)\n",
    "                    print(f\"Saved transaction data to {tx_output_file}\")\n",
    "            \n",
    "            return blocks_df, transactions_df\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "def process_multiple_files(file_list, max_blocks_per_file=None, save_individual=True, save_combined=True):\n",
    "\n",
    "    all_blocks = []\n",
    "    all_transactions = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\nProcessing file {os.path.basename(file_path)}...\")\n",
    "            print(f\"File size: {os.path.getsize(file_path) / (1024 * 1024):.2f} MB\")\n",
    "            \n",
    "            try:\n",
    "                blocks_df, transactions_df = extract_blockchain_data(\n",
    "                    file_path, \n",
    "                    max_blocks=max_blocks_per_file,\n",
    "                    save_output=save_individual\n",
    "                )\n",
    "                \n",
    "                if not blocks_df.empty:\n",
    "                    all_blocks.append(blocks_df)\n",
    "                if not transactions_df.empty:\n",
    "                    all_transactions.append(transactions_df)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nProcessing interrupted by user.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    combined_blocks = pd.concat(all_blocks, ignore_index=True) if all_blocks else pd.DataFrame()\n",
    "    combined_transactions = pd.concat(all_transactions, ignore_index=True) if all_transactions else pd.DataFrame()\n",
    "    \n",
    "    # remove duplicates\n",
    "    if not combined_blocks.empty:\n",
    "        combined_blocks = combined_blocks.drop_duplicates(subset=['block_number', 'block_hash'])\n",
    "    if not combined_transactions.empty:\n",
    "        combined_transactions = combined_transactions.drop_duplicates(subset=['tx_hash'])\n",
    "    \n",
    "    if not combined_transactions.empty and 'timestamp' in combined_transactions.columns:\n",
    "        missing_timestamps = combined_transactions['timestamp'].isna().sum()\n",
    "        if missing_timestamps > 0 and not combined_blocks.empty:\n",
    "            print(f\"Adding timestamps to {missing_timestamps} transactions that are missing them...\")\n",
    "            \n",
    "            block_timestamps = dict(zip(combined_blocks['block_number'], combined_blocks['timestamp']))\n",
    "            \n",
    "            def get_block_timestamp(row):\n",
    "                if pd.isna(row['timestamp']) and row['block_number'] in block_timestamps:\n",
    "                    return block_timestamps[row['block_number']]\n",
    "                return row['timestamp']\n",
    "            \n",
    "            combined_transactions['timestamp'] = combined_transactions.apply(get_block_timestamp, axis=1)\n",
    "            \n",
    "            # Check if we still have missing timestamps\n",
    "            still_missing = combined_transactions['timestamp'].isna().sum()\n",
    "            if still_missing > 0:\n",
    "                print(f\"Warning: {still_missing} transactions still missing timestamps\")\n",
    "            else:\n",
    "                print(\"All transactions now have timestamps\")\n",
    "    \n",
    "    # save combined \n",
    "    if save_combined:\n",
    "        if not combined_blocks.empty:\n",
    "            combined_blocks.to_csv(\"combined_blockchain_blocks.csv\", index=False)\n",
    "            print(f\"Saved combined block data: {len(combined_blocks)} unique blocks\")\n",
    "        \n",
    "        if not combined_transactions.empty:\n",
    "            combined_transactions.to_csv(\"combined_blockchain_transactions.csv\", index=False)\n",
    "            print(f\"Saved combined transaction data: {len(combined_transactions)} unique transactions\")\n",
    "    \n",
    "    return combined_blocks, combined_transactions\n",
    "\n",
    "def analyze_blockchain_data(blocks_df, transactions_df):\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    if not blocks_df.empty:\n",
    "        # Block-level analysis\n",
    "        results['block_count'] = len(blocks_df)\n",
    "        results['block_range'] = f\"{blocks_df['block_number'].min()} - {blocks_df['block_number'].max()}\"\n",
    "        results['avg_gas_used'] = blocks_df['gas_used'].mean()\n",
    "        results['avg_gas_limit'] = blocks_df['gas_limit'].mean()\n",
    "        results['gas_utilization'] = (blocks_df['gas_used'] / blocks_df['gas_limit']).mean() * 100\n",
    "        results['avg_block_size'] = blocks_df['size'].mean()\n",
    "        \n",
    "        # Time analysis\n",
    "        if 'timestamp' in blocks_df.columns:\n",
    "            blocks_df['datetime'] = pd.to_datetime(blocks_df['timestamp'], unit='s')\n",
    "            blocks_df = blocks_df.sort_values('block_number')\n",
    "            if len(blocks_df) > 1:\n",
    "                blocks_df['time_diff'] = blocks_df['timestamp'].diff()\n",
    "                results['avg_block_time'] = blocks_df['time_diff'].mean()\n",
    "    \n",
    "    if not transactions_df.empty:\n",
    "        # Transaction-level analysis\n",
    "        results['transaction_count'] = len(transactions_df)\n",
    "        \n",
    "\n",
    "        tx_per_block = transactions_df.groupby('block_number').size()\n",
    "        results['total_transactions_from_tx_data'] = tx_per_block.sum()\n",
    "        results['avg_transactions_per_block'] = tx_per_block.mean()\n",
    "        results['max_transactions_in_a_block'] = tx_per_block.max()\n",
    "        \n",
    "        #  avg gas price\n",
    "        results['avg_gas_price'] = pd.to_numeric(transactions_df['gas_price'], errors='coerce').mean()\n",
    "        \n",
    "\n",
    "            \n",
    "        # Transaction value analysis\n",
    "        if 'value' in transactions_df.columns:\n",
    "            if transactions_df['value'].dtype == 'object':\n",
    "                try:\n",
    "                    if str(transactions_df['value'].iloc[0]).startswith('0x'):\n",
    "                        transactions_df['value_numeric'] = transactions_df['value'].apply(\n",
    "                            lambda x: int(x, 16) if isinstance(x, str) and x.startswith('0x') else x\n",
    "                        )\n",
    "                    else:\n",
    "                        transactions_df['value_numeric'] = pd.to_numeric(transactions_df['value'], errors='coerce')\n",
    "                    \n",
    "                    results['avg_transaction_value'] = transactions_df['value_numeric'].mean()\n",
    "                    results['total_transaction_value'] = transactions_df['value_numeric'].sum()\n",
    "                    results['zero_value_tx_percentage'] = (transactions_df['value_numeric'] == 0).mean() * 100\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    " \n",
    "    if not blocks_df.empty and not transactions_df.empty:\n",
    "        blocks_with_tx = set(blocks_df['block_number'])\n",
    "        tx_blocks = set(transactions_df['block_number'])\n",
    "        \n",
    "        results['blocks_with_transactions'] = len(blocks_with_tx.intersection(tx_blocks))\n",
    "        results['blocks_missing_transactions'] = len(blocks_with_tx - tx_blocks)\n",
    "        \n",
    "\n",
    "        \n",
    "        if 'transaction_count' in blocks_df.columns:\n",
    "            blocks_tx_sum = blocks_df['transaction_count'].sum()\n",
    "            actual_tx_count = len(transactions_df)\n",
    "            results['block_reported_tx_count'] = blocks_tx_sum\n",
    "            results['actual_tx_count'] = actual_tx_count\n",
    "            results['tx_count_match'] = blocks_tx_sum == actual_tx_count\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = [\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17011001-17012000.json\",\n",
    "        \"C:/Users/haile/Downloads/170-171/blockTransactions17015001-17020000.json\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    blocks, transactions = process_multiple_files(files, max_blocks_per_file=None)\n",
    "    \n",
    "\n",
    "    if not blocks.empty or not transactions.empty:\n",
    "        analysis = analyze_blockchain_data(blocks, transactions)\n",
    "        \n",
    "        print(\"\\nBlockchain Data Analysis:\")\n",
    "        for key, value in analysis.items():\n",
    "            print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cf041-d886-4928-ba7a-66d6f1b0f040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
