# -*- coding: utf-8 -*-
"""Copy of Aamina_WeakLabeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cLc94SaHDPm5uN_5dwDvTybCaiSQDJ7H
"""

from google.colab import drive
# import os
drive.mount('/content/drive/')

# for the time being will
# blocks = conn.execute("select * from blocks_").fetch_df()

import pandas as pd
import glob
import os
from pathlib import Path

df_dir = Path('/content/drive/MyDrive/Unzipped_Data')
info_paths = list(df_dir.rglob('*_Block/*Block_Info.csv'))
print(f'Found {len(info_paths)} files:')
for p in info_paths:
    print('  ', p.relative_to(df_dir))

dfs = []
for p in info_paths:
    try:
        dfs.append(pd.read_csv(p, low_memory=False))
    except Exception as e:
        print(f"Couldn’t read {p.name}: {e}")

df_info = pd.concat(dfs, ignore_index=True)
print("Combined shape:", df_info.shape)

cols_to_drop = [
    'difficulty',
    'blobGasUsed',
    'excessBlobGas',
    'blobBaseFeePerGas',
    'blobTxCnt',
    'blobCnt',
]

df_info.drop(columns=cols_to_drop, errors='ignore', inplace=True)
df_info["Date"] = pd.to_datetime(df_info["timestamp"], unit='s').dt.date
display(df_info.tail())

display(df_info[df_info["blockNumber"] == 17000000])

import pandas as pd
file_path = '/content/drive/MyDrive/ETH_USD_sep2022.csv' # the second last day is all NaN, please clean it out
eth_price= pd.read_csv(file_path)
eth_price.head()
# conn.register('eth_price', eth_price)
# conn.execute("create or replace table eth_price as select * from eth_price")

df_info['Date']     = pd.to_datetime(df_info['Date'], errors='coerce')
eth_price['Date']   = pd.to_datetime(eth_price['Date'], errors='coerce')

df_info['Date']     = df_info['Date'].dt.normalize()
eth_price['Date']   = eth_price['Date'].dt.normalize()

merged = df_info.merge(
    eth_price,
    on = "Date", # 2024-05-20 TO 2025-04-12, POS SEPTEMBER 15TH 2022
    how = "inner"
)

merged

print(merged.dtypes)

print(eth_price[['Open',	'High',	'Low',	'Close',	'Adj Close',	'Volume']].describe(percentiles=[.25, .5, .75]) )

print( merged[['size', 'gasLimit', 'gasUsed', 'minGasPrice', 'maxGasPrice', 'avgGasPrice','txFees','baseFeePerGas','burntFees', 'tipsFees', 'transactionCount']].describe(percentiles=[.25, .5, .75]) )

for col in ['Open','High','Low','Close', 'Adj Close', 'Volume']:
  idx = merged[col].idxmax()
  print(f"Max {col:5s} = {merged.loc[idx, col]:.2f} on {merged.loc[idx,'Date']} , Blockchain Details: BaseFeePerGas {merged.loc[idx,'baseFeePerGas']}, Burn Fees {merged.loc[idx,'burntFees']}, tx Fees {merged.loc[idx,'txFees']} ,  Size {merged.loc[idx,'size']} ,TransactionCount {merged.loc[idx,'transactionCount']}, blockNumber {merged.loc[idx,'blockNumber']} ")


print("\n")

for col in ['Open','High','Low','Close', 'Adj Close', 'Volume']:
    idx = merged[col].idxmin()
    print(f"Min {col:5s} = {merged.loc[idx, col]:.2f} on {merged.loc[idx,'Date']} , Blockchain Details: BaseFeePerGas {merged.loc[idx,'baseFeePerGas']}, Burn Fees {merged.loc[idx,'burntFees']}, Tx Fees {merged.loc[idx,'txFees']} ,  Size {merged.loc[idx,'size']} ,TransactionCount {merged.loc[idx,'transactionCount']}, blockNumber {merged.loc[idx,'blockNumber']} ")

print("\n")


price_cols = ['Open','High','Low','Close','Adj Close','Volume']

# Compute the raw stats
stats = {}
for col in price_cols:
    stats[col] = {
        'mean':   merged[col].mean(),
        'median': merged[col].median(),
        'q25':    merged[col].quantile(0.25),
        'q75':    merged[col].quantile(0.75),
    }

def nearest_k_idxs(series: pd.Series, target: float, k: int):
    """
    Return the Index labels of the k values in series
    whose absolute difference from target is smallest.
    """
    diffs = (series - target).abs()
    # .nsmallest returns a Series of length k, with the same index labels
    return diffs.nsmallest(k).index

# how many “nearest” you want
K = 10000

for col in price_cols:
    print(f"\n=== {col} ===")
    for stat_name, stat_val in stats[col].items():
        idxs = nearest_k_idxs(merged[col], stat_val, K)
        print(f"\n  {stat_name.capitalize():6s} ≈ {stat_val:,.2f} → nearest {K} days:")
        for idx in idxs:
            row = merged.loc[idx]
            print(
                f" on{row['Date'].date()}  "
                f"{col}={row[col]:.2f}  "
                f"BaseFeePerGas={row['baseFeePerGas']:.2f}, "
                f"gasUsed={row['gasUsed']:.2f}, "
                f"burntFees={row['burntFees']:.2f}, "
                f"size={int(row['size'])}, "
                f"txCount={int(row['transactionCount'])}, "
                f"blockNumber={int(row['blockNumber'])}"
            )

K = 10000
price_cols = ['Open','High','Low','Close','Adj Close','Volume']
gold_rows = []

for col in price_cols:
    # —— MAX & nearest K —— #
    max_val  = merged[col].max()
    idxs_max = (merged[col] - max_val).abs().nsmallest(K).index

    print(f"\n=== MAX & {K} NEAREST for {col} (target={max_val:.2f}) ===")
    for idx in idxs_max:
        row = merged.loc[idx]
        date_str = row['Date'].strftime('%Y-%m-%d')
        print(
            f" on {date_str}  "
            f"{col}={float(row[col]):.2f}  "
            f"→ blockNumber={int(row['blockNumber'])}, "
            f"BaseFeePerGas={float(row['baseFeePerGas']):.2f}, "
            f"burntFees={float(row['burntFees']):.2f}, "
            f"txFees={float(row['txFees']):.2f}, "
            f"size={int(row['size'])}, "
            f"txCount={int(row['transactionCount'])}"
        )
        gold_rows.append({
            'blockNumber': int(row['blockNumber']),
            'label':       1,   # high
        })

    # —— MIN & nearest K —— #
    min_val  = merged[col].min()
    idxs_min = (merged[col] - min_val).abs().nsmallest(K).index

    print(f"\n=== MIN & {K} NEAREST for {col} (target={min_val:.2f}) ===")
    for idx in idxs_min:
        row = merged.loc[idx]
        date_str = row['Date'].strftime('%Y-%m-%d')
        print(
            f" on {date_str}  "
            f"{col}={float(row[col]):.2f}  "
            f"→ blockNumber={int(row['blockNumber'])}, "
            f"BaseFeePerGas={float(row['baseFeePerGas']):.2f}, "
            f"burntFees={float(row['burntFees']):.2f}, "
            f"txFees={float(row['txFees']):.2f}, "
            f"size={int(row['size'])}, "
            f"txCount={int(row['transactionCount'])}"
        )
        gold_rows.append({
            'blockNumber': int(row['blockNumber']),
            'label':       0,   # low
        })

gold_df = (
    pd.DataFrame(gold_rows)
      .drop_duplicates(subset='blockNumber')
      .reset_index(drop=True)
)

print("\nGold‐standard table:")
print(gold_df.head(), "\n… total rows:", len(gold_df))

# collect extra “quantile” rows
quantile_rows = []

# how many nearest to grab around each quantile
K = 10000

for col in price_cols:
    for quantile_name, label in [('q25', 0), ('q75', 1)]:
        target = stats[col][quantile_name]
        idxs = nearest_k_idxs(merged[col], target, K)
        print(f"\n=== {col} {quantile_name} (label={label}) → {K} nearest to {target:.2f} ===")
        for idx in idxs:
            row = merged.loc[idx]
            print(
                f"  on {row['Date'].date()}  {col}={row[col]:.2f}  "
                f"blockNumber={int(row['blockNumber'])}"
            )
            quantile_rows.append({
                'blockNumber': int(row['blockNumber']),
                'label':       label,
            })

# turn into a DataFrame (and drop duplicates in case a block shows up twice)
quantile_df = pd.DataFrame(quantile_rows).drop_duplicates(subset='blockNumber')

# And now append to your existing gold_df:
gold_df = pd.concat([gold_df, quantile_df], ignore_index=True)  \
               .drop_duplicates(subset='blockNumber')     \
               .reset_index(drop=True)

print(f"\nAfter including quantiles, gold‐standard has {len(gold_df)} rows")

#flushing just for checkinkg purposes
del gold_df

gold_df.tail()

# 1) Join your gold labels back to the full merged DataFrame
gs = pd.merge(
    gold_df,               # has blockNumber + label
    merged,                # your full DataFrame with all the block details
    on="blockNumber",
    how="left"
)

# 2) Select the on-chain detail columns you care about
detail_cols = [
    "baseFeePerGas",
    "gasUsed",
    "burntFees",
    "txFees",
    "size",
    "transactionCount",
]

gs[detail_cols].describe().T
gs.groupby("label")[detail_cols].describe().T

print(gs[detail_cols].dtypes)

for col in ["burntFees", "txFees"]:
    gs[col] = pd.to_numeric(gs[col], errors="coerce")

gs.head()

print(gold_df["label"].value_counts())

pip install snorkel

#figure out the type for each data value in both df_tx and df_info
# convert the types correctly (if need be)
# then LSTM

ABSTAIN = -1
HIGH = 1
LOW = 0

from snorkel.labeling import labeling_function

tx_median = merged['transactionCount'].median()
hi_tx = merged['transactionCount'].quantile(0.75)  # 75th percentile
lo_tx = merged['transactionCount'].quantile(0.25)  # 25th percentile

@labeling_function()
def lf_tx_count(row):
    # High transaction count
    if row.transactionCount > hi_tx * 1.5:
        return 1
    # Low transaction count
    elif row.transactionCount < lo_tx * 0.5:
        return 0
    else:
        return -1


#coefficient of variance
mean_gas = merged['avgGasPrice'].mean()
std_gas = merged['avgGasPrice'].std()
cv_gas = std_gas / mean_gas
merged['cv_gas'] = cv_gas


@labeling_function()
def lf_fee_dispersion(row):
    # High gas fee dispersion
    if row.cv_gas > 0.5:
        return 1
    elif row.cv_gas < 0.1:
        return 0
    else:
        return -1

#have to convert to numeric
merged['txFees'] = pd.to_numeric(df_info['txFees'], errors='coerce')
threshold_high_value = merged['txFees'].quantile(0.75)
threshold_low_value = merged['txFees'].quantile(0.25)


@labeling_function()
def lf_tx_fees(row):
    # High volatility if transaction fee is unusually high
    if row.txFees > threshold_high_value:
        return 1  # HIGH volatility
    elif row.txFees < threshold_low_value:
        return 0  # LOW volatility
    else:
        return -1

@labeling_function()
def lf_whale_value(row):
    # High volatility if transaction fees are greater than a defined threshold
    if row.txFees > 1e9:  # 1 billion
        return 1
    elif row.txFees < 1e6:  # 1 million
        return 0
    else:
        return -1



@labeling_function()
def lf_gas_utilisation(row):
    gas_utilisation = row.gasUsed / row.gasLimit
    # High volatility if gas usage is greater than 90%
    if gas_utilisation > 0.9:
        return 1
    # gas usage is less than 50%
    elif gas_utilisation < 0.5:
        return 0
    else:
        return -1  # Abstain





# @labeling_function()
# def lf_base_fee_jump(row, prev_row):
#     # Calculate percentage change in base fee
#     fee_change = abs(row.baseFeePerGas - prev_row.baseFeePerGas) / prev_row.baseFeePerGas

#     # High volatility if the fee change is greater than 50%
#     if fee_change > 0.5:
#           return 1
#     # Low volatility if fee change is less than 10%
#     elif fee_change < 0.1:
#         return 0
#     else:
#         return -1



# Group by blockNumber and count unique minerAddress (senders) per block
# merged['unique_senders'] = df_info.groupby('blockNumber')['minerAddress'].transform(lambda x: x.nunique())

# @labeling_function()
# def lf_unique_senders(row):
#     # High volatility if there are more than 100 unique senders
#     if row.unique_senders > 100:
#         return 1
#     elif row.unique_senders < 10:
#         return 0
#     else:
#         return -1

# @labeling_function()
# def lf_base_fee_jump(row):
#     # Compare the baseFeePerGas of the current row with a predefined threshold
#     threshold = 0.2
#     mean_base_fee = merged['baseFeePerGas'].mean()

#     if abs(row.baseFeePerGas - mean_base_fee) / mean_base_fee > threshold:
#         return 1  # High volatility (base fee jump)
#     elif abs(row.baseFeePerGas - mean_base_fee) / mean_base_fee < threshold:
#         return 0
#     else:
#         return -1

import numpy as np

def custom_sample(df, sample_frac, head_frac, tail_frac, middle_frac):
    # Determine the number of rows for each portion
    total_rows = len(df)

    # Get head and tail portions based on fractions
    head_rows = int(total_rows * head_frac)
    tail_rows = int(total_rows * tail_frac)

    # Get random sample from the middle portion (the remaining portion after head and tail)
    middle_rows = int(total_rows * middle_frac)

    # Head: Get the first few rows
    head_sample = df.head(head_rows)

    # Tail: Get the last few rows
    tail_sample = df.tail(tail_rows)

    # Middle: Randomly sample from the middle portion of the DataFrame
    middle_start = head_rows
    middle_end = total_rows - tail_rows
    middle_sample = df.iloc[middle_start:middle_end].sample(n=middle_rows, random_state=42)

    # Combine the samples and shuffle them to ensure randomness
    # sampled_df = pd.concat([head_sample, tail_sample, middle_sample]).sample(frac=1, random_state=42).reset_index(drop=True)
    sampled_df = pd.concat([head_sample, middle_sample, tail_sample]).reset_index(drop=True)

    return sampled_df

df_sample = custom_sample(merged, sample_frac=0.3, head_frac=0.001, tail_frac=0.001, middle_frac=0.001)



from snorkel.labeling.model import LabelModel
from snorkel.labeling import PandasLFApplier

lfs = [lf_tx_count, lf_base_fee_jump, lf_gas_utilisation, lf_whale_value, lf_tx_fees, lf_fee_dispersion]

applier = PandasLFApplier(lfs)
L_train = applier.apply(df_sample)

# Train the label model and compute the training labels
label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)

df_sample["label"] = label_model.predict(L=L_train, tie_break_policy="abstain")

from snorkel.labeling import LFAnalysis

gold_arr = gold_df['label'].to_numpy()
lf_analysis = LFAnalysis(L=L_train, lfs=lfs)

# 2) Get a summary table as a pandas DataFrame
# summary_df = lf_analysis.lf_summary()

# print(summary_df)



LFAnalysis(L_train).lf_summary(gold_arr).sample(5)

print( merged[['size', 'gasLimit', 'gasUsed', 'minGasPrice', 'maxGasPrice', 'avgGasPrice','txFees','baseFeePerGas','burntFees', 'tipsFees', 'transactionCount']].describe(percentiles=[.25, .5, .75]) )

import numpy as np
import pandas as pd
# NEW LABELING FUNCTIONS ----------


#figure out the type for each data value in both df_tx and df_info
# convert the types correctly (if need be)
# then LSTM

ABSTAIN = -1
HIGH = 1
LOW = 0

from snorkel.labeling import labeling_function

# tx_median = merged['transactionCount'].median()
# tx_mean= merged['transactionCount'].mean()
# tx_std = merged["transactionCount"].std()
# hi_tx = merged['transactionCount'].quantile(0.75)  # 75th percentile
# lo_tx = merged['transactionCount'].quantile(0.25)  # 25th percentile
# #can be quite spread for both high and low

# @labeling_function()
# def lf_tx_count(row):
#   # mu=tx_mean, sigma=tx_std, k=1.0):
#     # z = (row.transactionCount - mu) / sigma
#     # if z > k:
#     #     return 1
#     # elif z < -k:
#     #     return 0
#     # else:
#     #     return -1

#     # High transaction count

#     if row.transactionCount > tx_median or row.transactionCount > tx_mean:
#         return 1

#     # Low transaction count
#     elif row.transactionCount < tx_median and row.transactionCount < tx_mean:
#       return 0
#     else:
#         return -1


tx_25, tx_75 = np.quantile(merged["transactionCount"], [0.25, 0.75])
iqr = tx_75 - tx_25
# we’ll say “high” if > median + 0.5*IQR, “low” if < median - 0.5*IQR
med = merged["transactionCount"].median()
lo_iqr = med - 0.5 * iqr
hi_iqr = med + 0.5 * iqr

# Z‐score thresholds
tx_mean, tx_std = merged["transactionCount"].mean(), merged["transactionCount"].std()
k_z = 1.5   # flag anything >1.5σ away

# Delta‐based thresholds
merged = merged.sort_values("blockNumber")
merged["delta_tx"] = merged["transactionCount"].diff() / merged["transactionCount"].shift()
p_lo, p_hi = merged["delta_tx"].quantile([0.25, 0.75])

# ——————————————————————————————————————————————
@labeling_function(name="lf_tx_count_improved")
def lf_tx_count_improved(row,
                         lo_iqr: float = lo_iqr,
                         hi_iqr: float = hi_iqr,
                         mu: float = tx_mean,
                         sigma: float = tx_std,
                         k_z: float = k_z,
                         lo_d: float = p_lo,
                         hi_d: float = p_hi) -> int:
    x = row.transactionCount

    # 1) IQR‐based
    if x >= hi_iqr:
        return 1
    if x <= lo_iqr:
        return 0

    # 2) Z‐score tails
    # z = (x - mu) / sigma
    # if z >= k_z:
    #     return 1
    # if z <= -k_z:
    #     return 0

    # 3) Sudden jump/drop from previous block
    d = row.delta_tx
    if d >= hi_d:
        return 1
    if d <= lo_d:
        return 0

    # otherwise, abstain
    return -1




# larger size means higher volatilitiy
large_size = merged['size'].quantile(0.75)  # 75th percentile
small_size = merged['size'].quantile(0.30)

@labeling_function()
def lf_size(row):
    if row.size > large_size:
        return 1
    # Low transaction count
    elif row.size < small_size:
        return 0
    else:
        return -1


#coefficient of variance
mean_gas = merged['avgGasPrice'].mean()
std_gas = merged['avgGasPrice'].std()
cv_gas = std_gas / mean_gas
merged['cv_gas'] = cv_gas


merged["dispersion"] = (
    merged["maxGasPrice"] - merged["minGasPrice"]
) / merged["avgGasPrice"]

p25_disp = merged["dispersion"].quantile(0.25)
p75_disp = merged["dispersion"].quantile(0.75)

@labeling_function()
def lf_fee_dispersion(row, lo=p25_disp, hi=p75_disp):
    if row.dispersion >= hi:
        return HIGH
    elif row.dispersion <= lo:
        return LOW
    return ABSTAIN



# @labeling_function()
# def lf_fee_dispersion(row):
#     # High gas fee dispersion
#     if row.cv_gas > 0.5:
#         return 1
#     elif row.cv_gas < 0.1:
#         return 0
#     else:
#         return -1

#have to convert to numeric
merged['txFees'] = pd.to_numeric(df_info['txFees'], errors='coerce')
threshold_high_value = merged['txFees'].quantile(0.75)
threshold_low_value = merged['txFees'].quantile(0.25)


@labeling_function()
def lf_tx_fees(row):
    # High volatility if transaction fee is unusually high
    if row.txFees > threshold_high_value:
        return 1  # HIGH volatility
    elif row.txFees < threshold_low_value:
        return 0  # LOW volatility
    else:
        return -1


# 50%
tx_fees_percentile = merged['txFees'].quantile(0.5)
@labeling_function()
def lf_whale_value(row):
    # High volatility if transaction fees are greater than a defined threshold
    if row.txFees > tx_fees_percentile:
        return 1
    elif row.txFees < tx_fees_percentile:  # 1 million
        return 0
    else:
        return -1



@labeling_function()
def lf_gas_utilisation(row):
    gas_utilisation = row.gasUsed / row.gasLimit
    # High volatility if gas usage is greater than 90%
    if gas_utilisation > 0.9:
        return 1
    # gas usage is less than 50%
    elif gas_utilisation < 0.5:
        return 0
    else:
        return -1  # Abstain


 # delta from the previous block
 #spikes and drops are seen more easily
merged = merged.sort_values("blockNumber")
merged["delta_baseFee"] = merged["baseFeePerGas"].diff() / merged["baseFeePerGas"].shift()
p75_delta   = merged["delta_baseFee"].quantile(0.75)
p25_delta   = merged["delta_baseFee"].quantile(0.25)

@labeling_function()
def lf_base_fee_jump_delta(row, lo=p25_delta, hi=p75_delta):
    if row.delta_baseFee >= hi:
        return 1
    elif row.delta_baseFee <= lo:
        return 0
    return -1

#0.47 and 0.53

# p25 = merged["baseFeePerGas"].quantile(0.25)
# p75 = merged["baseFeePerGas"].quantile(0.75)

# @labeling_function()
# def lf_base_fee_jump(row, low_cutoff: float = p25, high_cutoff: float = p75) -> int:
#     """
#     Label 1 if baseFeePerGas is in the top 25% (a big jump),
#     0 if it's in the bottom 25% (a big drop),
#     else abstain.
#     """
#     fee = row.baseFeePerGas
#     if fee >= high_cutoff:
#         return 1
#     elif fee <= low_cutoff:
#         return 0
#     else:
#         return -1

from snorkel.labeling import PandasLFApplier, LFAnalysis

# 1) Build df_dev = only those rows in merged_df that appear in gold_df
df_dev = merged[merged["blockNumber"].isin(gold_df["blockNumber"])].copy()

# 2) Merge in the true labels so they line up 1:1
df_dev = df_dev.merge(
    gold_df[["blockNumber", "label"]],
    on="blockNumber",
    how="inner",
)

# 3) Extract the gold‐label array
gold_arr = gold_df['label'].to_numpy()
# Y_dev = df_dev["label"].to_numpy()

lfs = [lf_tx_count, lf_base_fee_jump, lf_gas_utilisation, lf_whale_value, lf_tx_fees, lf_fee_dispersion]

# 4) Apply LFs to df_dev
applier = PandasLFApplier(lfs)
L_dev = applier.apply(df_dev)


# sanity check
print(L_dev.shape, gold_arr.shape)


summary_df = LFAnalysis(L_dev).lf_summary(gold_arr)
summary_df.sample(5)

lfs = [lf_tx_count_improved, lf_size, lf_fee_dispersion, lf_tx_fees, lf_whale_value, lf_gas_utilisation, lf_base_fee_jump_delta]

# 4) Apply LFs to df_dev
applier = PandasLFApplier(lfs)
L_dev = applier.apply(df_dev)


# sanity check
print(L_dev.shape, gold_arr.shape)


LFAnalysis(L_dev).lf_summary(gold_arr).sample(7)



# print("L_train shape:", L_train.shape)
print("L_dev   shape:", L_dev.shape)

from snorkel.labeling.model import LabelModel
from snorkel.labeling import PandasLFApplier

label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train=L_dev, n_epochs=500, log_freq=50, seed=123)

# merged["label"] = label_model.predict(L=L_train, tie_break_policy="abstain")

# preds_dev = label_model.predict(L=L_dev, tie_break_policy="abstain")
# results = df_dev.copy()
# results["predicted_label"] = preds_dev
# # your true labels column was named "label" when you merged gold_df in
# results = results[["blockNumber", "label", "predicted_label"]]
# display(results)

preds_dev = label_model.predict(
    L=L_dev,
    tie_break_policy="abstain"  # returns -1 wherever the model is uncertain
)

probs_dev = label_model.predict_proba(L=L_dev)
# probs_dev is an array of shape (n_examples, cardinality),
# e.g. [[0.1, 0.9], [0.8, 0.2], …]

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

results = df_dev.copy()
results["predicted_label"] = preds_dev

# If you want to ignore abstains:
mask = results["predicted_label"] != -1
y_true = results.loc[mask, "label"]
y_pred = results.loc[mask, "predicted_label"]

print("Accuracy:", accuracy_score(y_true, y_pred))
print("Classification Report:\n", classification_report(y_true, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

# And to just glance at the table:
display(results[["blockNumber", "label", "predicted_label"]])

import pandas as pd

results = []
for i, lf in enumerate(lfs):
    preds = L_dev[:, i]
    mask = preds != -1
    if not mask.any():
        continue
    acc = (preds[mask] == gold_arr[mask]).mean()
    cov = mask.mean()  # fraction of rows non-abstained
    results.append({
        "LF": lf.name,
        "Coverage": f"{cov:.2f}",
        "Accuracy": f"{acc:.2f}",
        "Votes": mask.sum(),
    })

print(pd.DataFrame(results).sort_values("Accuracy", ascending=False))

label_model.predict(L=L_train, tie_break_policy="abstain")
# 2) Mask out any abstains
mask = preds != -1
preds_masked = preds[mask]
gold_masked  = Y_dev[mask]

# 3) Compute accuracy
acc = accuracy_score(gold_masked, preds_masked)
print(f"LabelModel accuracy = {acc:.2%}")

# 4) Print confusion matrix
print("Confusion matrix:")
print(confusion_matrix(gold_masked, preds_masked))

# 5) Full classification report
print("\nClassification report:")
print(classification_report(gold_masked, preds_masked))

preds_full = label_model.predict(L=L_train, tie_break_policy="abstain")

# mask_full = df_train["blockNumber"].isin(gold_df["blockNumber"])
# preds_gold = preds_full[mask_full]
# gold_gold  = df_train.loc[mask_full, "blockNumber"].map(
#     gold_df.set_index("blockNumber")["label"]
# ).to_numpy()

print(preds_full)

from snorkel.labeling import PandasLFApplier

lfs = [
    lf_tx_count, lf_size, lf_fee_dispersion, lf_tx_fees,
    lf_whale_value, lf_gas_utilisation, lf_base_fee_jump_delta
]

# 2) Build the labeling matrix over your full merged table
applier = PandasLFApplier(lfs)
L_full = applier.apply(merged)  # shape: (n_rows, n_LFs)

# 3) Predict weak labels for every row
preds_full = label_model.predict(
    L=L_full,
    tie_break_policy="abstain"    # yields -1 where the model abstains
)

# 4) Add them back to your DataFrame
merged["weak_label"] = preds_full

# Quick sanity checks:
print("Label distribution:\n", merged["weak_label"].value_counts(dropna=False))
display(merged.head())

pip install duckdb

import duckdb
db_path = '/content/drive/MyDrive/eth1.duckdb'
conn = duckdb.connect(database=db_path, read_only=False)

conn.execute("show tables").fetch_df()

conn.register("merged_", merged)

conn.execute(
    """
    CREATE OR REPLACE TABLE merged AS
    SELECT *
    FROM merged_;
    """
)


df_merged = conn.execute("SELECT * FROM merged").fetchdf()

df_merged = conn.execute("SELECT * FROM merged").fetchdf()

df_merged.head()

conn.close()